{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTS-notBTS classification for the Zwicky Transient Facility (ZTF) using deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import datetime\n",
    "from astropy.time import Time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
    "\n",
    "import os\n",
    "import io\n",
    "import gzip\n",
    "from astropy.io import fits\n",
    "from bson.json_util import loads, dumps\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use(['dark_background'])\n",
    "from pandas.plotting import register_matplotlib_converters, scatter_matrix\n",
    "register_matplotlib_converters()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "#     \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 14,\n",
    "})\n",
    "plt.rcParams['axes.linewidth'] = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_pd_gr(trips, cand):\n",
    "    cand['isdiffpos'] = [True if isdiffpos == 't' else False for isdiffpos in cand['isdiffpos']]\n",
    "    \n",
    "    # Diagnostics\n",
    "    # iband = cand[cand['fid']==3]\n",
    "    # neg_iband = iband[~iband['isdiffpos']]\n",
    "\n",
    "    # iband_objids = iband['objectId'].value_counts().index.to_numpy()\n",
    "    # iband_counts = iband['objectId'].value_counts().to_numpy()\n",
    "\n",
    "    # cand_objids = cand['objectId'].value_counts().index.to_numpy()\n",
    "    # cand_counts = cand['objectId'].value_counts().to_numpy()\n",
    "\n",
    "    # print(f\"Percent of alerts that are in the i-band {100*len(iband)/len(cand):.2f}%\")\n",
    "    # print(f\"Percent of objects that have at least one i band alert {100*len(iband_objids)/len(cand_objids):.2f}%\")\n",
    "\n",
    "    # neg_iband_objids = neg_iband['objectId'].value_counts().index.to_numpy()\n",
    "    # neg_iband_counts = neg_iband['objectId'].value_counts().to_numpy()\n",
    "\n",
    "    # print(f\"Percent of i-band alerts that have negative differences {100*len(neg_iband)/len(iband):.2f}%\")\n",
    "\n",
    "    cand_pd_gr = cand[(cand['isdiffpos']) & ((cand['fid'] == 1) | (cand['fid'] == 2))]\n",
    "    triplets_pd_gr = trips[(cand['isdiffpos']) & ((cand['fid'] == 1) | (cand['fid'] == 2))]\n",
    "\n",
    "    # print(\"Positive difference alerts in g- or r-band\", len(cand_pd_gr))\n",
    "    return triplets_pd_gr, cand_pd_gr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(set_names, cuts, name, N_max=None, seed=2):\n",
    "    np.random.seed(seed)\n",
    "    # concat\n",
    "    # optionally save to disk? with provided name\n",
    "    triplets = np.empty((0,63,63,3))\n",
    "    cand = pd.DataFrame()\n",
    "\n",
    "    for set_name in set_names:\n",
    "        print(f\"Working on {set_name} data\")\n",
    "        # load set\n",
    "        set_trips = np.load(f\"data/base_data/{set_name}_triplets.npy\", mmap_mode='r')\n",
    "        set_cand = pd.read_csv(f\"data/base_data/{set_name}_candidates.csv\", index_col=False)\n",
    "        print(\"  Read\")\n",
    "        \n",
    "        # run other optional cuts (ex: take only positive differences in g or r band)\n",
    "        set_trips, set_cand = cuts(set_trips, set_cand)\n",
    "        print(\"  Ran cuts\")\n",
    "        set_cand.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # thin to N_max\n",
    "        # plt.figure()\n",
    "        # _ = plt.hist(set_cand['objectId'].value_counts(), histtype='step', bins=50)\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        print(f\"  Initial median of {np.median(set_cand['objectId'].value_counts())} detections per object\")\n",
    "        \n",
    "        if N_max is not None:\n",
    "            drops = np.empty((0,), dtype=int)\n",
    "            for ID in set(set_cand['objectId']):\n",
    "                reps = np.argwhere(np.asarray(set_cand['objectId']) == ID).flatten()\n",
    "                if len(reps) >= N_max:\n",
    "                    drops = np.concatenate((drops, np.random.choice(reps, len(reps)-N_max, replace=False)))\n",
    "            \n",
    "            set_trips = np.delete(set_trips, drops, axis=0)\n",
    "            set_cand = set_cand.drop(index=drops)\n",
    "            set_cand.reset_index(inplace=True)\n",
    "            print(f\"  Dropped {len(drops)} {set_name} alerts down to {N_max} max per obj\")\n",
    "        \n",
    "        # concat\n",
    "        triplets = np.concatenate((triplets, set_trips))\n",
    "        cand = pd.concat((cand, set_cand))\n",
    "        print(f\"  Merged {set_name}\")\n",
    "\n",
    "    # or return?\n",
    "    np.save(f\"data/triplets_{name}{ f'_{N_max}max' if N_max is not None else '' }.npy\", triplets)\n",
    "    cand.to_csv(f\"data/candidates_{name}{ f'_{N_max}max' if N_max is not None else '' }.csv\", index=False)\n",
    "    print(\"Wrote merged triplets and candidate data\")\n",
    "    del triplets, cand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_max = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_train_data(['bts_true', 'bts_false', 'MS'], only_pd_gr, name=\"pd_gr\", N_max=N_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'data/candidates_pd_gr_{N_max}max.csv')\n",
    "# display(df)\n",
    "# df.info()\n",
    "# df.describe()\n",
    "print(f'num_notbts: {np.sum(df.label == 0)}')\n",
    "print(f'num_bts: {np.sum(df.label == 1)}')\n",
    "\n",
    "# We will use memory mapping as the file is relatively large (1 GB)\n",
    "triplets = np.load(f'data/triplets_pd_gr_{N_max}max.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = np.random.randint(0, high=len(triplets), size=5)\n",
    "# for ii in ind:\n",
    "#     print(f'candid: {df.loc[ii, \"candid\"]}, label: {df.loc[ii, \"label\"]}')\n",
    "#     plot_triplet(triplets[ii, :], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# color_wheel = {0: \"#dc3545\", \n",
    "#                1: \"#28a745\"}\n",
    "# colors = df[\"label\"].map(lambda x: color_wheel.get(x))\n",
    "\n",
    "# columns = ['magpsf', 'fwhm', 'ndethist', 'scorr']\n",
    "\n",
    "# axx = scatter_matrix(df.loc[df.label >= 0, columns], color=[color_wheel[label] for label in df.label],\n",
    "#                      alpha=0.2,  diagonal='hist', ax=ax, grid=True,\n",
    "#                      hist_kwds={'color': 'darkblue', 'alpha': 0, 'bins': 50})\n",
    "\n",
    "# for rc in range(len(columns)):\n",
    "#     rc_y_max = 0\n",
    "#     for group in color_wheel.keys():\n",
    "#         y = df[df.label == group][columns[rc]]\n",
    "#         hh = axx[rc][rc].hist(y, bins=50, alpha=0.5, color=color_wheel[group], density=1)\n",
    "# #         print(np.min(hh[0]), np.max(hh[0]))\n",
    "#         rc_y_max = max(rc_y_max, np.max(hh[0]))\n",
    "#         axx[rc][rc].set_ylim([0, 1.1*rc_y_max])\n",
    "\n",
    "# # scatter_matrix(df.loc[df.label == 0, ['magpsf', 'fwhm', 'ndethist']], \n",
    "# #                alpha=0.2,  diagonal='hist', ax=ax,\n",
    "# #                hist_kwds={'color': '#dc3545', 'alpha': 0.5, 'bins': 100}, color='#dc3545')\n",
    "# # scatter_matrix(df.loc[df.label == 1, ['magpsf', 'fwhm', 'ndethist']], \n",
    "# #                alpha=0.2,  diagonal='hist', ax=ax,\n",
    "# #                hist_kwds={'color': '#28a745', 'alpha': 0.5, 'bins': 100}, color='#28a745')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = df['jd'].map(lambda x: Time(x, format='jd').datetime)\n",
    "# fig = plt.figure(figsize=(7, 3), dpi=100)\n",
    "# ax = fig.add_subplot(111)\n",
    "# notbts = ax.hist(df.loc[df['label'] == 0, 'date'], bins=50, range=(17500,19100), #linestyle='dashed',\n",
    "#         color=color_wheel[0], histtype='step', label='notBTS', linewidth=1.2)\n",
    "# bts = ax.hist(df.loc[df['label'] == 1, 'date'], bins=50, range=(17500,19100),\n",
    "#         color=color_wheel[1], histtype='step', label='BTS', linewidth=1.2)\n",
    "# # ax.set_xlabel('Date')\n",
    "# ax.set_ylabel('Count')\n",
    "# ax.legend(loc='best')\n",
    "# ax.grid(True, linewidth=.3)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 81\\% / 9\\% / 10\\% training/validation/test data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_split = 0.1  # fraction of all data\n",
    "random_state = 2\n",
    "\n",
    "ztfids_seen, ztfids_test = train_test_split(pd.unique(df['objectId']), test_size=test_split, random_state=random_state)\n",
    "\n",
    "# Want array of indices for training alerts and testing alerts\n",
    "# Need to shuffle because validation is bottom 10% of train - shuffle test as well for consistency\n",
    "is_seen = df['objectId'].isin(ztfids_seen)\n",
    "is_test = ~is_seen\n",
    "mask_seen = shuffle(df.index.values[is_seen], random_state=random_state)\n",
    "mask_test  = shuffle(df.index.values[is_test], random_state=random_state)\n",
    "\n",
    "x_seen, y_seen = triplets[mask_seen], df['label'][mask_seen]\n",
    "x_test,  y_test  = triplets[mask_test] , df['label'][mask_test]\n",
    "\n",
    "num_seen_obj = len(ztfids_seen)\n",
    "num_test_obj = len(ztfids_test)\n",
    "num_obj = len(pd.unique(df['objectId']))\n",
    "print(f\"{num_seen_obj} seen/train+val objects\")\n",
    "print(f\"{num_test_obj} unseen/test objects\")\n",
    "print(f\"{100*(num_seen_obj/num_obj):.2f}%/{100*(num_test_obj/num_obj):.2f}% seen/unseen split by object\\n\")\n",
    "\n",
    "num_seen_alr = len(x_seen)\n",
    "num_test_alr = len(x_test)\n",
    "num_alr = len(df['objectId'])\n",
    "print(f\"{num_seen_alr} seen/train+val alerts\")\n",
    "print(f\"{num_test_alr} unseen/test alerts\")\n",
    "print(f\"{100*(num_seen_alr/num_alr):.2f}%/{100*(num_test_alr/num_alr):.2f}% seen/unseen split by alert\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.1  # fraction of the seen data\n",
    "\n",
    "ztfids_train, ztfids_val = train_test_split(ztfids_seen, test_size=validation_split, random_state=random_state)\n",
    "\n",
    "is_train = df['objectId'].isin(ztfids_train)\n",
    "is_val = df['objectId'].isin(ztfids_val)\n",
    "mask_train = shuffle(df.index.values[is_train], random_state=random_state)\n",
    "mask_val  = shuffle(df.index.values[is_val], random_state=random_state)\n",
    "\n",
    "x_train, y_train = triplets[mask_train], df['label'][mask_train]\n",
    "x_val, y_val = triplets[mask_val], df['label'][mask_val]\n",
    "\n",
    "num_train_obj = len(ztfids_train)\n",
    "num_val_obj = len(ztfids_val)\n",
    "num_obj = len(pd.unique(df['objectId']))\n",
    "print(f\"{num_train_obj} train objects\")\n",
    "print(f\"{num_val_obj} val objects\")\n",
    "print(f\"{100*(num_train_obj/num_obj):.2f}%/{100*(num_val_obj/num_obj):.2f}% train/val split by object\\n\")\n",
    "\n",
    "num_train_alr = len(x_train)\n",
    "num_val_alr = len(x_val)\n",
    "num_alr = len(df['objectId'])\n",
    "print(f\"{num_train_alr} train alerts\")\n",
    "print(f\"{num_val_alr} val alerts\")\n",
    "print(f\"{100*(num_train_alr/num_alr):.2f}%/{100*(num_val_alr/num_alr):.2f}% train/val split by alert\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = shuffle(triplets[mask_train], df['label'][mask_train], random_state=random_state)\n",
    "# x_test, y_test   = shuffle(triplets[mask_test], df['label'][mask_test], random_state=random_state)\n",
    "\n",
    "# x_train, y_train = triplets[mask_train], df['label'][mask_traintrain_mask]\n",
    "# x_test, y_test = triplets[mask_test], df['label'][mask_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `braai` architecture\n",
    "\n",
    "We will use a simple custom VGG-like sequential model ($VGG6$; this architecture was first proposed by the Visual Geometry Group of the Department of Engineering Science, University of Oxford, UK). The model has six layers with trainable parameters: four convolutional and two fully-connected. The first two convolutional layers use 16 3x3 pixel filters each while in the second pair, 32 3x3 pixel filters are used. To prevent over-fitting, a dropout rate of 0.25 is applied after each max-pooling layer and a dropout rate of 0.5 is applied after the second fully-connected layer. ReLU activation functions (Rectified Linear Unit --  a function defined as the positive part of its argument) are used for all five hidden trainable layers; a sigmoid activation function is used for the output layer.\n",
    "\n",
    "![](img/fig-braai2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def vgg6(input_shape=(63, 63, 3), n_classes: int = 1):\n",
    "    \"\"\"\n",
    "        VGG6\n",
    "    :param input_shape:\n",
    "    :param n_classes:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential(name='VGG6')\n",
    "    # input: 63x63 images with 3 channel -> (63, 63, 3) tensors.\n",
    "    # this applies 16 convolution filters of size 3x3 each.\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, name='conv1'))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', name='conv2'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25, name='drop_0.25'))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', name='conv3'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', name='conv4'))\n",
    "    model.add(MaxPool2D(pool_size=(4, 4)))\n",
    "    model.add(Dropout(0.25, name='drop2_0.25'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256, activation='relu', name='fc_1'))\n",
    "    model.add(Dropout(0.4, name='drop3_0.4'))\n",
    "    # output layer\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "    model.add(Dense(n_classes, activation=activation, name='fc_out'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def vgg16(input_shape=(63, 63, 3), n_classes: int = 1):\n",
    "    # https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c\n",
    "    \n",
    "    model = tf.keras.models.Sequential(name='VGG16')\n",
    "    # input: 63x63 images with 3 channel -> (63, 63, 3) tensors.\n",
    "    # this applies 16 convolution filters of size 3x3 each.\n",
    "    model.add(Conv2D(input_shape=input_shape, filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv1'))\n",
    "    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", name='conv2'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv3'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv4'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv5'))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv6'))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv7'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv8'))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv9'))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv10'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv11'))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv12'))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv13'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units=4096,activation=\"relu\"))\n",
    "    model.add(Dense(units=4096,activation=\"relu\"))\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "    model.add(Dense(units=n_classes, activation=activation))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def vgg_thin_reduce(input_shape=(63, 63, 3), n_classes: int = 1):    \n",
    "    model = tf.keras.models.Sequential(name='VGG16')\n",
    "    # input: 63x63 images with 3 channel -> (63, 63, 3) tensors.\n",
    "    # this applies 16 convolution filters of size 3x3 each.\n",
    "    model.add(Conv2D(input_shape=input_shape, filters=16, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv1'))\n",
    "    model.add(Conv2D(filters=16,kernel_size=(3,3),padding=\"same\", activation=\"relu\", name='conv2'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv3'))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv4'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv5'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv6'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv7'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "#     model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv8'))\n",
    "#     model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv9'))\n",
    "#     model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv10'))\n",
    "#     model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "#     model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv11'))\n",
    "#     model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv12'))\n",
    "#     model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv13'))\n",
    "#     model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units=512,activation=\"relu\"))\n",
    "#     model.add(Dense(units=4096,activation=\"relu\"))\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "    model.add(Dense(units=n_classes, activation=activation))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "`braai` is implemented using `TensorFlow` software and its high-level `Keras` API. We will use the binary cross-entropy loss function, the Adam optimizer, a batch size of 64, and a 81\\%/9\\%/10\\% training/validation/test data split. The training image data are weighted per class to mitigate the real vs. bogus imbalance in the data sets. To augment the data, the images may be flipped horizontally and/or vertically at random. No random rotations and translations will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(path: str = './', stamp: str = None, report: dict = dict()):\n",
    "    f_name = os.path.join(path, f'report.{stamp}.json')\n",
    "    with open(f_name, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "# make train and test masks:\n",
    "# _, _, mask_train, mask_test = train_test_split(df.label, list(range(len(df.label))),\n",
    "#                                                    test_size=test_split, random_state=random_state)\n",
    "\n",
    "\n",
    "masks = {'training': mask_train, 'val': mask_val, 'test': mask_test}\n",
    "\n",
    "# print(mask_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2=x_train[10:11,:,:]\n",
    "x_val2=x_val[10:11,:,:]\n",
    "\n",
    "y_train2=y_train.to_numpy()[0:1]\n",
    "y_val2=y_val.to_numpy()[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'adam'\n",
    "epochs = 5\n",
    "patience = 50\n",
    "class_weight = True\n",
    "batch_size = 64\n",
    "\n",
    "# halt training if no gain in validation accuracy over patience epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "data_augmentation = {'horizontal_flip': True,\n",
    "                     'vertical_flip': True,\n",
    "                     'fill_mode': 'constant',\n",
    "                     'cval': 1e-9,\n",
    "                     'rotation': True\n",
    "                    }\n",
    "\n",
    "preprocess_func = lambda img: np.rot90(img, np.random.choice([-1, 0, 1, 2])) if data_augmentation['rotation'] else lambda img: img\n",
    "\n",
    "                     \n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=data_augmentation['horizontal_flip'],\n",
    "                                                          vertical_flip  =data_augmentation['vertical_flip'],\n",
    "                                                          fill_mode      =data_augmentation['fill_mode'],\n",
    "                                                          cval           =data_augmentation['cval'],\n",
    "                                                          preprocessing_function=preprocess_func)\n",
    "\n",
    "training_generator = datagen.flow(x_train, y_train, batch_size=1, save_to_dir=\"imgs/\")\n",
    "validation_generator = datagen.flow(x_val, y_val, batch_size=1, save_to_dir=\"imgs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(x_train2))\n",
    "print(np.shape(y_train2))\n",
    "\n",
    "print(np.shape(x_val2))\n",
    "print(np.shape(y_val2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classification = True if loss == 'binary_crossentropy' else False\n",
    "n_classes = 1 if binary_classification else 2\n",
    "\n",
    "# training data weights\n",
    "if class_weight:\n",
    "    # weight data class depending on number of examples?\n",
    "    if not binary_classification:\n",
    "        num_training_examples_per_class = np.sum(y_train, axis=0)\n",
    "    else:\n",
    "        num_training_examples_per_class = np.array([len(y_train) - np.sum(y_train), np.sum(y_train)])\n",
    "\n",
    "    assert 0 not in num_training_examples_per_class, 'found class without any examples!'\n",
    "\n",
    "    # fewer examples -- larger weight\n",
    "    weights = (1 / num_training_examples_per_class) / np.linalg.norm((1 / num_training_examples_per_class))\n",
    "    normalized_weight = weights / np.max(weights)\n",
    "\n",
    "    class_weight = {i: w for i, w in enumerate(normalized_weight)}\n",
    "\n",
    "else:\n",
    "    class_weight = {i: 1 for i in range(2)}\n",
    "    \n",
    "# image shape:\n",
    "image_shape = x_train.shape[1:]\n",
    "print('Input image shape:', image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg6(input_shape=image_shape, n_classes=n_classes)\n",
    "\n",
    "# set up optimizer:\n",
    "if optimizer == 'adam':\n",
    "    optimzr = tf.keras.optimizers.Adam(learning_rate=3e-4, beta_1=0.9, beta_2=0.999,\n",
    "                                       epsilon=None, decay=0.0, amsgrad=False)\n",
    "elif optimizer == 'sgd':\n",
    "    optimzr = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "else:\n",
    "    print('Could not recognize optimizer, using Adam')\n",
    "    optimzr = tf.keras.optimizers.Adam(learning_rate=3e-4, beta_1=0.9, beta_2=0.999,\n",
    "                                       epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=optimzr, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(triplets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_t_stamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f'{model.name}_{run_t_stamp}'\n",
    "\n",
    "h = model.fit(training_generator,\n",
    "              steps_per_epoch=0.8*len(x_train) // batch_size,\n",
    "              validation_data=validation_generator,\n",
    "              validation_steps=(0.8*len(x_test)) // batch_size,\n",
    "              class_weight=class_weight,\n",
    "              epochs=epochs,\n",
    "              verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate the resulting model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating on training set to check misclassified samples:')\n",
    "labels_training_pred = model.predict(x_train, batch_size=batch_size, verbose=1)\n",
    "# XOR will show misclassified samples\n",
    "misclassified_train_mask = np.array(list(map(int, df.label[masks['training']]))).flatten() ^ \\\n",
    "                           np.array(list(map(int, np.rint(labels_training_pred)))).flatten()\n",
    "\n",
    "misclassified_train_mask = [ii for ii, mi in enumerate(misclassified_train_mask) if mi == 1]\n",
    "\n",
    "misclassifications_train = {int(c): [int(l), float(p)]\n",
    "                            for c, l, p in zip(df.candid.values[masks['training']][misclassified_train_mask],\n",
    "                                               df.label.values[masks['training']][misclassified_train_mask],\n",
    "                                               labels_training_pred[misclassified_train_mask])}\n",
    "# print(misclassifications_train)\n",
    "\n",
    "print('Evaluating on validation set for loss and accuracy:')\n",
    "preds = model.evaluate(x_val, y_val, batch_size=batch_size, verbose=1)\n",
    "val_loss = float(preds[0])\n",
    "val_accuracy = float(preds[1])\n",
    "print(\"Loss = \" + str(val_loss))\n",
    "print(\"Val Accuracy = \" + str(val_accuracy))\n",
    "\n",
    "print('Evaluating on validation set to check misclassified samples:')\n",
    "preds = model.predict(x=x_val, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# XOR will show misclassified samples\n",
    "misclassified_val_mask = np.array(list(map(int, df.label[masks['val']]))).flatten() ^ \\\n",
    "                          np.array(list(map(int, np.rint(preds)))).flatten()\n",
    "misclassified_val_mask = [ii for ii, mi in enumerate(misclassified_val_mask) if mi == 1]\n",
    "\n",
    "misclassifications_val = {int(c): [int(l), float(p)]\n",
    "                           for c, l, p in zip(df.candid.values[masks['val']][misclassified_val_mask],\n",
    "                                              df.label.values[masks['val']][misclassified_val_mask],\n",
    "                                              preds[misclassified_val_mask])}\n",
    "\n",
    "# round probs to nearest int (0 or 1)\n",
    "labels_pred = np.rint(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dir = \"models/\"+model_name+\"/\"\n",
    "model_dir = report_dir+\"model/\"\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(df['label'][masks['val']], preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "CFD = ConfusionMatrixDisplay.from_predictions(df.label.values[masks['val']], \n",
    "                                        labels_pred, normalize='true', ax=ax)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = np.array(list(map(int, df.label[masks['val']]))).flatten()\n",
    "val_rpreds = np.array(list(map(int, np.rint(preds)))).flatten()\n",
    "\n",
    "val_TP_mask = np.bitwise_and(val_labels, val_rpreds)\n",
    "val_TN_mask = 1-(np.bitwise_or(val_labels, val_rpreds))\n",
    "val_FP_mask = np.bitwise_and(1-val_labels, val_rpreds)\n",
    "val_FN_mask = np.bitwise_and(val_labels, 1-val_rpreds)\n",
    "\n",
    "val_TP_idxs = [ii for ii, mi in enumerate(val_TP_mask) if mi == 1]\n",
    "val_TN_idxs = [ii for ii, mi in enumerate(val_TN_mask) if mi == 1]\n",
    "val_FP_idxs = [ii for ii, mi in enumerate(val_FP_mask) if mi == 1]\n",
    "val_FN_idxs = [ii for ii, mi in enumerate(val_FN_mask) if mi == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per object model accuracy for val objects in g, r bands\n",
    "val_perobj_g_acc = np.zeros(len(ztfids_val))\n",
    "val_perobj_r_acc = np.zeros(len(ztfids_val))\n",
    "\n",
    "for i, ztfid in enumerate(ztfids_val): \n",
    "    cands = df[df['objectId']==ztfid]\n",
    "    label = cands['label'].to_numpy()[0]\n",
    "\n",
    "    g_cands = cands[cands['fid']==1]\n",
    "    g_trips = triplets[df['objectId']==ztfid][cands['fid']==1]\n",
    "\n",
    "    r_cands = cands[cands['fid']==2]\n",
    "    r_trips = triplets[df['objectId']==ztfid][cands['fid']==2]\n",
    "\n",
    "    if len(g_cands) > 0:\n",
    "        g_preds = np.array(np.rint(model.predict(g_trips).flatten()), dtype=int)\n",
    "        val_perobj_g_acc[i] = np.sum(g_preds==label)/len(g_trips)\n",
    "    else:\n",
    "        g_preds = []\n",
    "        val_perobj_g_acc[i] = -1\n",
    "     \n",
    "    if len(r_cands) > 0:\n",
    "        r_preds = np.array(np.rint(model.predict(r_trips).flatten()), dtype=int)\n",
    "        val_perobj_r_acc[i] = np.sum(r_preds==label)/len(r_trips)\n",
    "    else:\n",
    "        r_preds = []\n",
    "        val_perobj_r_acc[i] = -1\n",
    "    \n",
    "#     print(len(cands), label)\n",
    "#     print(len(g_cands), len(r_cands))\n",
    "#     print(g_preds, r_preds)\n",
    "#     print(val_perobj_g_acc[i], val_perobj_r_acc[i])\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training report in json format\n",
    "print('Generating report...')\n",
    "r = {'Run time stamp': run_t_stamp,\n",
    "     'Model name': model_name,\n",
    "     'Model trained': 'vgg6',\n",
    "     'Batch size': batch_size,\n",
    "     'Optimizer': optimizer,\n",
    "     'Requested number of train epochs': epochs,\n",
    "     'Early stopping after epochs': patience,\n",
    "     'Training+validation/test split': test_split,\n",
    "     'Training/validation split': validation_split,\n",
    "     'Weight training data by class': class_weight,\n",
    "     'Random state': random_state,\n",
    "     'Number of training examples': x_train.shape[0],\n",
    "     'Number of val examples': x_val.shape[0],\n",
    "     'X_train shape': x_train.shape,\n",
    "     'Y_train shape': y_train.shape,\n",
    "     'X_val shape': x_val.shape,\n",
    "     'Y_val shape': y_val.shape,\n",
    "     'Data augmentation': data_augmentation,\n",
    "     'Confusion matrix': CFD.confusion_matrix.tolist(),\n",
    "     'Misclassified val candids': list(misclassifications_val.keys()),\n",
    "     'Misclassified training candids': list(misclassifications_train.keys()),\n",
    "     'Val misclassifications': misclassifications_val,\n",
    "     'Training misclassifications': misclassifications_train,\n",
    "     'Training history': h.history\n",
    "     }\n",
    "for k in r['Training history'].keys():\n",
    "    r['Training history'][k] = np.array(r['Training history'][k]).tolist()\n",
    "\n",
    "# print(r)\n",
    "\n",
    "save_report(path=report_dir, stamp=run_t_stamp, report=r)\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(report_dir+'model_summary.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'), expand_nested=True, show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'accuracy' in h.history:\n",
    "    train_acc = h.history['accuracy']\n",
    "    val_acc = h.history['val_accuracy']\n",
    "else:\n",
    "    train_acc = h.history['acc']\n",
    "    val_acc = h.history['val_acc']\n",
    "\n",
    "train_loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3, figsize=(15, 8), dpi=250)\n",
    "\n",
    "ax1.plot(train_acc, label='Training', linewidth=2)\n",
    "ax1.plot(val_acc, label='Validation', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_ylim([0.6,0.9])\n",
    "ax1.grid(True, linewidth=.3)\n",
    "\n",
    "ax2.plot(train_loss, label='Training', linewidth=2)\n",
    "ax2.plot(val_loss, label='Validation', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend(loc='best')\n",
    "ax2.set_ylim([0.621,0.63])\n",
    "ax2.grid(True, linewidth=.3)\n",
    "\n",
    "bins = np.arange(12,22,0.5)\n",
    "ax3.hist(df['magpsf'][masks['val']].to_numpy()[val_TP_idxs], histtype='step', color='g', linewidth=2, label='TP', bins=bins, zorder=2)\n",
    "ax3.hist(df['magpsf'][masks['val']].to_numpy()[val_TN_idxs], histtype='step', color='b', linewidth=2, label='TN', bins=bins, zorder=3)\n",
    "ax3.hist(df['magpsf'][masks['val']].to_numpy()[val_FP_idxs], histtype='step', color='r', linewidth=2, label='FP', bins=bins, zorder=4)\n",
    "ax3.hist(df['magpsf'][masks['val']].to_numpy()[val_FN_idxs], histtype='step', color='orange', linewidth=2, label='FN', bins=bins, zorder=5)\n",
    "ax3.axvline(18.5, c='k', linewidth=2, linestyle='dashed', label='18.5', alpha=0.5, zorder=10)\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.set_xlabel('Magnitude')\n",
    "ax3.set_ylim([0,int(len(x_val)/5)])\n",
    "ax3.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "\n",
    "ax4.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n",
    "ax4.set_xlim([0.0, 1.0])\n",
    "ax4.set_ylim([0.0, 1.05])\n",
    "ax4.plot(fpr, tpr, lw=2, label=f'ROC (area = {roc_auc:.5f})')\n",
    "ax4.set_xlabel('False Positive Rate (Contamination)')\n",
    "ax4.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax4.legend(loc=\"lower right\")\n",
    "ax4.grid(True, linewidth=.3)\n",
    "ax4.set(aspect='equal')\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(df.label.values[masks['val']], \n",
    "                                        labels_pred, normalize='true', \n",
    "                                        display_labels=[\"notBTS\", \"BTS\"], \n",
    "                                        cmap=plt.cm.Blues, colorbar=False, ax=ax5)\n",
    "\n",
    "hist, xbins, ybins, im = ax6.hist2d(val_perobj_g_acc, val_perobj_r_acc, norm=LogNorm(), bins=4, range=[[0,1],[0,1]])\n",
    "ax6.set_xlabel('Per-object g-band accuracy')\n",
    "ax6.set_ylabel('Per-object r-band accuracy')\n",
    "ax6.set(aspect='equal')\n",
    "ax6.xaxis.set_major_locator(MultipleLocator(0.25))\n",
    "ax6.yaxis.set_major_locator(MultipleLocator(0.25))\n",
    "\n",
    "for i in range(len(ybins)-1):\n",
    "    for j in range(len(xbins)-1):\n",
    "        ax6.text(xbins[j]+0.14,ybins[i]+0.115, f\"{100*int(hist.T[i,j])/len(ztfids_val):.1f}%\", color=\"w\", ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "for ax in [ax1, ax2, ax3, ax4, ax5, ax6]:\n",
    "    ax.tick_params(which='both', width=1.5)\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(report_dir+\"fig.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(\"models/VGG6_20220723_004850/model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot_lightcurve_class(ztfid):\n",
    "    fid_to_color = {\n",
    "        1: 'green',\n",
    "        2: 'red',\n",
    "        3: 'orange'\n",
    "    }\n",
    "    \n",
    "    fid_to_band = {\n",
    "        1: 'g',\n",
    "        2: 'r',\n",
    "        3: 'i'\n",
    "    }\n",
    "    \n",
    "    cands = df[df['objectId']==ztfid]\n",
    "    trips = triplets[df['objectId']==ztfid]\n",
    "        \n",
    "    first_detect = cands['jd'].min()\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(8,5), dpi=200)\n",
    "    \n",
    "    for t_idx, c_idx in enumerate(cands.index):\n",
    "        cand = cands.loc[c_idx]\n",
    "        trip = trips[t_idx:t_idx+1]\n",
    "        jd_since_fd = cand['jd'] - first_detect\n",
    "        if int(np.rint(model.predict(trip)[0][0])):\n",
    "            # predicted as BTS\n",
    "            plt.errorbar(jd_since_fd, cand['magpsf'], fmt='*', ms=12, fillstyle='none', color=fid_to_color[cand['fid']], yerr=cand['sigmapsf'])\n",
    "        else:\n",
    "            # predicted as not BTS\n",
    "            plt.errorbar(jd_since_fd, cand['magpsf'], fmt='s', fillstyle='none', color=fid_to_color[cand['fid']], yerr=cand['sigmapsf'])\n",
    "    \n",
    "    plt.plot([],[], marker='*', color='gray', fillstyle='none', label='BTS')\n",
    "    plt.plot([],[], marker='s', color='gray', fillstyle='none', label='notBTS')\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(\"days since first detection\", size=16, labelpad=10)\n",
    "    ax.set_ylabel(\"PSF magnitude\", size=16, labelpad=10)\n",
    "    ax.legend()#, bbox_to_anchor=(1.2,1))\n",
    "    ax.tick_params(which='both', width=1.5)\n",
    "    ax.set_title()\n",
    "    \n",
    "    # change if statement\n",
    "    # change plot labels\n",
    "    # must run with N_max > 1 or read in all alerts for object\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "ztfid = ztfids_val[49]\n",
    "print(ztfid)\n",
    "fig, ax = plot_lightcurve_class(ztfid)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical advice from one of DL godfathers Andrej Karpathy\n",
    "\n",
    "Highly recommended: [Andrej Karpathy's blog post](http://karpathy.github.io/2019/04/25/recipe/)\n",
    "\n",
    "- Neural net training is a leaky abstraction\n",
    "\n",
    "```bash\n",
    ">>> your_data = # plug your awesome dataset here\n",
    ">>> model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)\n",
    "# conquer world here\n",
    "```\n",
    "\n",
    "- Neural net training fails silently\n",
    "\n",
    "Lots of ways to screw things up -> many paths to pain and suffering\n",
    "\n",
    "#### The recipe\n",
    "\n",
    "- Become one with the data\n",
    "    - probably, the most important and time consuming step\n",
    "    - visualize as much as you can\n",
    "    - check normalizations\n",
    "    \n",
    "    \n",
    "The neural net is effectively a compressed/compiled version of your dataset, you'll be able to look at your network (mis)predictions and understand where they might be coming from. And if your network is giving you some prediction that doesn't seem consistent with what you've seen in the data, something is off.\n",
    "\n",
    "\n",
    "- Set up the end-to-end training/evaluation skeleton + get dumb baselines\n",
    "    - fix random seed\n",
    "    - simplify\n",
    "    - add significant digits to your eval\n",
    "    - init well\n",
    "    - fancy loss func? verify at init\n",
    "    - verify decreasing training loss\n",
    "    - visualize just before the net\n",
    "    - active learning: check model's misclassifications, both training and testing\n",
    "\n",
    "\n",
    "- Overfit: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss).\n",
    "    - picking the model: don't be a hero\n",
    "    - adam is safe\n",
    "    - complexify only one at a time\n",
    "\n",
    "\n",
    "- Regularize\n",
    "    - get more data\n",
    "    - data augment\n",
    "    - smaller model size\n",
    "    - batchnorm\n",
    "    - decrease the batch size\n",
    "    - use dropout\n",
    "    - early stopping\n",
    "\n",
    "\n",
    "- Tune\n",
    "    - first prefer random over grid search\n",
    "    - hyper-parameter optimization. Check out [https://github.com/keras-team/keras-tuner](keras-tuner)\n",
    "\n",
    "\n",
    "- Squeeze out the juice\n",
    "    - ensembles\n",
    "    - leave it training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_index_col(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    display(df)\n",
    "    df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    df.drop(\"index\", axis=1, inplace=True)\n",
    "    display(df)\n",
    "    df.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_index_col('data/MS_candidates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
